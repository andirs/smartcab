{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a self-driving car agent\n",
    "## Reinforcement Learning Project\n",
    "This documentation outlines the thought process behind training a smartcab to navigate on its own. We're applying reinforcement learning techniques for a self-driving agent in a simplified world to aid it in effectively reaching its destinations in the allotted time. \n",
    "\n",
    "First we'll investigate the given environment in which the agent operates in and instruct it, to execute very basic driving commands. Afterwards we'll exhibit the different states a smartcab can be in, analyzing its limited world and derive a Q-Learning algorithm, that will guide the agent towards its destination. At last we'll be going through several iterations in order to find the best configuration for our algorithm and the environment its operating in, to improve the results.\n",
    "\n",
    "## Description\n",
    "In the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents — known as smartcabs — to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to rely on smartcabs to get to where they need to go as safely and efficiently as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or efficient as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and efficiency can be achieved.\n",
    "\n",
    "## Software Requirements\n",
    "This project uses the following software and Python libraries:\n",
    "\n",
    "* [Python 2.7](https://www.python.org/download/releases/2.7/)\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [PyGame](http://pygame.org/)\n",
    "    * **Helpful links for installing PyGame:**\n",
    "    * [Getting Started](https://www.pygame.org/wiki/GettingStarted)\n",
    "    * [PyGame Information](http://www.pygame.org/wiki/info)\n",
    "    * [Google Group](https://groups.google.com/forum/#!forum/pygame-mirror-on-google-groups)\n",
    "    * [PyGame subreddit](https://www.reddit.com/r/pygame/)\n",
    "    \n",
    "## Definitions\n",
    "\n",
    "### Environment\n",
    "The smartcab operates in an ideal, simplified, grid-like city (see image below). Roads are established on two major axis (North-South & East-West). There are other vehicles on the road but the city is abstracted to a point where there are no other obstacles such as traffic jams, construction sights or other heterogenous agents such as pedestrians. However there are certain rules the city runs on, which are alike to our street system. At each intersection there is a traffic light that either allows traffic in the North-South direction or the East-West direction.\n",
    "\n",
    "![Smart Cab Environment](smartcab_screenshot.png)\n",
    "\n",
    "**Following rules guide the traffic:**\n",
    "* On a green light, a left turn is permitted if there is no oncoming traffic making a right turn or coming straight through the intersection.\n",
    "* On a red light, a right turn is permitted if no oncoming traffic is approaching from your left through the intersection.\n",
    "\n",
    "### Inputs and Outputs\n",
    "Very much alike to modern hail-a-cab applications the smartcab will have a route assigned based on t he passengers' starting location and the destination. The route is split at each intersection into waypoints, and for simplicity purposes the the smartcab is at some intersection at any instant in the world. \n",
    "\n",
    "Therefore, the next waypoint to the destination, assuming the destination has not already been reached, is one intersection away in one direction (North, South, East, or West). \n",
    "\n",
    "### States\n",
    "The smartcab has only an egocentric view of the intersection it is at and can therefore use following information: \n",
    "\n",
    "* The state of the traffic light for its direction of movement: `['green', 'red']`\n",
    "* Whether there is a vehicle at the intersection for each of the oncoming directions: `['left', 'right', 'oncoming']`\n",
    "\n",
    "### Actions\n",
    "For each action, the smartcab has one of the following options:\n",
    "\n",
    "* Idle at the intersection\n",
    "* Drive to the next intersection to either of the directions, which offers a set of following actions: `[None, 'left', 'right', 'forward']`\n",
    "\n",
    "### Deadline\n",
    "The smartcab has to get to its final destination in a given time. With each action taken, this time decreases. If the allotted time becomes zero before reaching the destination, the trip has failed.\n",
    "\n",
    "### Rewards, Penalties and Goal\n",
    "Smartcabs receive rewards for each successfully completed trip and smaller rewards for each action they execute successfully that obeys traffic rules. For any incorrect action a small penalty will be given and violating traffic rules or causing traffic accidents result in a high penalty. Based on the rewards and penalties the smartcab receives, the self-driving agent implementation will learn an optimal policy for driving on the city roads while obeying traffic rules, avoiding accidents, and reaching passengers' destinations in the allotted time.\n",
    "\n",
    "## Implementing a Basic Driving Agent\n",
    "In order to get going, we'll implement a basic driving agent that chooses from all possible actions one at random and drives through the city streets. \n",
    "In order to do so, we set `enforce_deadline` to `False` on line `47` and implement the following code in `agent.py` on line `30`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick a random action from set of possible actions. \n",
    "import random\n",
    "action = random.choice([None, 'forward', 'left', 'right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartcab performs following 5 actions: ['right', 'forward', None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# Quick code to demonstrate effect of actions allocation. \n",
    "n = 5\n",
    "action_dict = []\n",
    "for _ in range(n):\n",
    "    action_dict.append(random.choice([None, 'forward', 'left', 'right']))\n",
    "print \"Smartcab performs following\", n, \"actions:\", action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the observations countable we're implementing a code that counts each state where `deadline == 0` and `reward < 8` as not successful and states in which `reward >= 8` as successful attempts. We're choosing `8` as a reward boundary in order to account for possible negative rewards that are possible on the last attempt to get to the goal. For example if the car gets into an accident while going on the last field. Following code simulates this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no_success</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>success</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            trial_count\n",
       "no_success           90\n",
       "success              10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create dict for trial summary\n",
    "trial_summary = {}\n",
    "# Initiate two states (0 = not successful, 1 = successful)\n",
    "for i in range(2):\n",
    "    trial_summary[i] = 0\n",
    "\n",
    "for i in range(100):\n",
    "    # Initiate dummy variables\n",
    "    deadline = 0\n",
    "    reward = 0\n",
    "    \n",
    "    # Simulates 10% of attempts being successful\n",
    "    if (i % 10 == 0):\n",
    "        deadline = 1\n",
    "        reward = 8\n",
    "    if (deadline == 0) & (reward < 8):\n",
    "        trial_summary[0] += 1\n",
    "    elif (reward >= 8):\n",
    "        trial_summary[1] += 1\n",
    "\n",
    "success_summary = pd.DataFrame.from_dict(trial_summary, orient='index')\n",
    "success_summary.columns = ['trial_count']\n",
    "success_summary.index = ['no_success', 'success']\n",
    "success_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned we'll implement a loop that executes the trials n-times, store the results in a table and takes the average over all n attempts for better comparision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`basic_agent.py` looks like follows:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "from environment import Agent, Environment\n",
    "from planner import RoutePlanner\n",
    "from simulator import Simulator\n",
    "\n",
    "class LearningAgent(Agent):\n",
    "    \"\"\"An agent that learns to drive in the smartcab world.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(LearningAgent, self).__init__(env)  # sets self.env = env, state = None, next_waypoint = None, and a default color\n",
    "        self.color = 'taxi'  # override color\n",
    "        self.planner = RoutePlanner(self.env, self)  # simple route planner to get next_waypoint\n",
    "        # TODO: Initialize any additional variables here\n",
    "        self.trial_count = 0\n",
    "        self.trial_summary = {}\n",
    "        for i in range(2):\n",
    "            self.trial_summary[i] = 0\n",
    "\n",
    "    def reset(self, destination=None):\n",
    "        self.planner.route_to(destination)\n",
    "        # TODO: Prepare for a new trip; reset any variables here, if required\n",
    "        self.trial_count = 0\n",
    "        #self.trial_summary[0] = 0\n",
    "        #self.trial_summary[1] = 0\n",
    "\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "\n",
    "        # TODO: Update state\n",
    "        \n",
    "        # TODO: Select action according to your policy\n",
    "        action = random.choice([None, 'forward', 'left', 'right'])\n",
    "\n",
    "        # Execute action and get reward\n",
    "        reward = self.env.act(self, action)\n",
    "\n",
    "        # TODO: Learn policy based on state, action, reward\n",
    "\n",
    "        print \"LearningAgent.update(): deadline = {}, inputs = {}, action = {}, reward = {}\".format(\n",
    "            deadline, inputs, action, reward)  # [debug]\n",
    "\n",
    "        if (deadline == 0) & (reward < 8):\n",
    "            self.trial_summary[0] += 1\n",
    "            print \"#\" * 20\n",
    "            print \"Trial was unsuccessful.\"\n",
    "            print \"#\" * 20\n",
    "        else:\n",
    "            if (reward >= 8):\n",
    "                self.trial_summary[1] += 1\n",
    "                print \"#\" * 20\n",
    "                print \"Trial was successful.\"\n",
    "                print \"#\" * 20\n",
    "\n",
    "def run():\n",
    "    \"\"\"Run the agent for a finite number of trials.\"\"\"\n",
    "\n",
    "    # Set up environment and agent\n",
    "    e = Environment()  # create environment (also adds some dummy traffic)\n",
    "    a = e.create_agent(LearningAgent)  # create agent\n",
    "    e.set_primary_agent(a, enforce_deadline=False)  # specify agent to track\n",
    "    # NOTE: You can set enforce_deadline=False while debugging to allow longer trials\n",
    "\n",
    "    # Now simulate it\n",
    "    # create simulator (uses pygame when display=True, if available)\n",
    "    sim = Simulator(e, update_delay=0, display=False)  \n",
    "    # NOTE: To speed up simulation, reduce update_delay and/or set display=False\n",
    "\n",
    "    success_summary = pd.DataFrame(index = ['no_success', 'success'])\n",
    "    validation_no = 10\n",
    "\n",
    "    for i in range(validation_no):\n",
    "        sim.run(n_trials=100)  # run for a specified number of trials\n",
    "        # NOTE: To quit midway, press Esc or close pygame window, or hit Ctrl+C on the command-line\n",
    "        print \"Trial Count: \", a.trial_count\n",
    "        success_temp = pd.DataFrame.from_dict(a.trial_summary, orient='index')\n",
    "        success_temp.index = ['no_success', 'success']\n",
    "        temp_column_name = 'trial_count_' + str(i+1)\n",
    "        #success_temp.columns = [temp_column_name]\n",
    "        success_summary[temp_column_name] = success_temp\n",
    "        a.trial_summary[0] = 0\n",
    "        a.trial_summary[1] = 0\n",
    "    #print \"Arrived\", Counter(a.trial_summary.values()).most_common()[1][1], \"times.\"\n",
    "    print success_summary\n",
    "    success_average = success_summary.mean(axis=1)\n",
    "    print \"Average: \"\n",
    "    print success_average\n",
    "    print \"Percentage: \", success_average[0:][1] / success_average[0:][0] \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "> There are a couple interesting observations when you run the smartcab with nothing but random actions:\n",
    "\n",
    "> * When crashing, cars will restart at a different location and start to perform actions again.\n",
    "> * There are 3 other agents or cars in play.\n",
    "> * With n $\\rightarrow \\infty$ (through `enforce_deadline = False`) the agent reaches the goal eventually if it doesn't crash into another vehicle.\n",
    "> * There is a reward system in place that shows the immediate gratification of an action.\n",
    "\n",
    "> When we set `enforce_deadline` to `False` and let it run 10 times with `n_trials = 100` we get following averaging stats:\n",
    "\n",
    "> |               | trial_count   |\n",
    "> | ------------- | ------------- |\n",
    "> | no_success    | 79.5          |\n",
    "> | success       | 20.5          |\n",
    "\n",
    "> Which indicates that only 25.78 % of all agents reach their destination in the given time. Not very convenient for our passengers. This is the measure we would like to improve significantly down the road. \n",
    "The data corresponding to this claim can be found here: `smartcab/data/basic_agent_trials.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inform the Driving Agent\n",
    "Now that the driving agent is capable of moving around in the environment, the next task is to identify a set of states that are appropriate for modeling the smartcab and environment. \n",
    "\n",
    "The main source of state variables are the current inputs at the intersection, but given our set of rules not all may require representation. The goal is, to process the inputs and update the agent's current state at each waypoint using the `self.state` variable. In order to check our performance we'll continue with the simulation deadline enforcement `enforce_deadline` being set to `False`.\n",
    "\n",
    "#### All information available\n",
    "Let's take a look at all information we've got available in our environment and talk through the possibilities that come with each of these inputs.\n",
    "\n",
    "##### Traffic Lights\n",
    "As mentioned above, we have an indicator that shows what color the traffic light at any given position of the agent has. This can take the values `green` and `red`. We find this information in the inputs dictionary and can call it with `inputs['light']`. Given our set of rules, this is a highly useful indicator for our smartcab to decide whether it should perform a certain action or not. This information will be included in our set of state variables.\n",
    "\n",
    "```python\n",
    "* `inputs['light'] = ['red', 'green']`\n",
    "```\n",
    "\n",
    "##### Traffic\n",
    "Our agent can detect, whether there is a vehicle at the intersection for each of the oncoming directions and what action this other agent is performing. Given the set of rules that apply on US streets we have following information that we need to especially emphasize:\n",
    "\n",
    "* If the traffic light is green:\n",
    "    * Is there oncoming traffic going in my direction? ($\\rightarrow$ can't perform a left turn)\n",
    "* If the traffic light is red:\n",
    "    * Is there traffic coming from the left side? ($\\rightarrow$ can't make a right turn)\n",
    "\n",
    "However since we want to let our agent learn these rules through reinforcement and gratification we are going to add all the information about traffic in our set of states. This adds following set to the list: \n",
    "\n",
    "```python\n",
    "* `inputs['oncoming'] = [None, 'left', 'right', 'forward']`\n",
    "* `inputs['left'] = [None, 'left', 'right', 'forward']`\n",
    "* `inputs['right'] = [None, 'left', 'right', 'forward']`\n",
    "```\n",
    "\n",
    "##### Waypoints\n",
    "The decision, which step to perform next is essential to learning if some action is good or bad, given the state. Especially in reinforcement learning. If you compare it to a simple but hurtful early life analogy, this is where our agent will understand if the cooktop is hot or not.\n",
    "\n",
    "```python\n",
    "* `self.next_waypoint = ['forward', 'left', 'right']`\n",
    "```\n",
    "\n",
    "##### Deadline\n",
    "One could argue that the time that is available for the smartcab to perform its delivery describes the state its in. We can think about classic situations where cab drivers are pushed to their limits and into breaking the law in order to get a passenger faster to the destination. Since we're building this system though, to perform optimal results without breaking the law we should discard this input though. \n",
    "Another valid concern is the number of possible combinations. Given that the deadline reduces by `1` on each action we perform an average of 40 to 50 actions each run, this would add a multiplier of `len(deadline)` to the state combinations that we already have. With only 100 training runs including around 40 to 50 actions in each run this might be a state input that rather distracts the learner from learning what's essential for the task.\n",
    "\n",
    "##### The final states for our Q-Learner\n",
    ">To summarize, we're planning on including following inputs to the list of possible state combinations:\n",
    "\n",
    "> ```python\n",
    "* `inputs['light'] = ['red', 'green']` -> 2\n",
    "* `inputs['oncoming'] = [None, 'left', 'right', 'forward']` -> 4\n",
    "* `inputs['left'] = [None, 'left', 'right', 'forward']` -> 4\n",
    "* `inputs['right'] = [None, 'left', 'right', 'forward']` -> 4\n",
    "* `self.next_waypoint = ['forward', 'left', 'right']` -> 3\n",
    "```\n",
    "\n",
    "The total number of states therefore is the combination of all states:\n",
    "$$ 2 * 4 * 4 * 3 = 384 $$\n",
    "\n",
    "**Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state?**\n",
    "\n",
    ">Given that we are dealing with around `5000` waypoints in our training runs (`100` runs * `~50` actions), the amount of `384` states seems to be a bit high. We could consider dropping `inputs['right']`, since given our rules if the traffic light is red and we want to make a right turn (which is allowed by US rules) this would be a correct action that wouldn't end up in an accident but all other actions would be incorrect and therefore should be learned as being incorrect. This would bring down the number of states to `96` but we would lose some flexibility, if we for example would change the rules slightly and appply it to a European traffic setting our Reinforcement Learner would learn incorrect rules. Since we'll be adding each state as a combination when it occures to a dictionary and calculate its given value we might not even end up evaluating and adding all `384` states. Therefore it should be alright to add all appearances out of `384` possible combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implement a Q-Learning Driving Agent\n",
    "With the driving agent being capable of interpreting inputs and having a mapping of environmental states, we now want to make her understand how to best choose actions given this information. In order to do so, we are implementing an algorithm called Q-Learning. \n",
    "Q-Learning will essentially compute the best action based on rewards given by states that the agent goes through. The Q-Learning driving agent will need to consider these rewards when updating its Q-values.\n",
    "\n",
    "Specialty: Stochastic vs. Deterministic\n",
    "\n",
    "The algorithm can be described as follows:\n",
    "\n",
    "1. Initialize $Q(s, a)$ arbitrarily\n",
    "2. For life or until learning is stopped...\n",
    "    3. Initialize $s$\n",
    "    4. Repeat (for each step of episode):\n",
    "        5. Choose $a$ from $s$ using policy derived from $Q$ (using i.e. ε greedy)\n",
    "        6. Take action $a$, observe reward $r$ at state $s'$\n",
    "        7. Update: $Q(s,a) \\leftarrow Q(s,a) + \\alpha * [r + \\gamma * (max_{\\alpha}Q(s',a') − Q(s,a))]$\n",
    "        8. $s \\leftarrow s'$;\n",
    "    9. Until $s$ is terminal\n",
    "\n",
    "\n",
    "* Where $\\alpha$ is the learning rate set between `[0, 1]`.\n",
    "* $\\gamma$ is the discount factor which models the fact that future rewards are worth less than immediate rewards.\n",
    "\n",
    "[Input from Faculty of Engineering UNSW](http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html)\n",
    "\n",
    "**ε greedy strategy**\n",
    "\n",
    "The ε-greedy strategy is to select the greedy action (one that maximizes Q[s,a]) all but ε of the time and to select a random action ε of the time, where 0 ≤ ε ≤ 1. It is possible to change ε through time. Intuitively, early in the life of the agent it should select a more random strategy to encourage initial exploration and, as time progresses, it should act more greedily.\n",
    "\n",
    "One problem with an ε-greedy strategy is that it treats all of the actions, apart from the best action, equivalently. If there are two seemingly good actions and more actions that look less promising, it may be more sensible to select among the good actions: putting more effort toward determining which of these promising actions is best, rather than putting in effort to explore the actions that look bad. One way to do that is to select action a with a probability depending on the value of Q[s,a]. This is known as a soft-max action selection.\n",
    "[AI - Foundations of Computational Agents](http://artint.info/html/ArtInt_266.html)\n",
    "\n",
    "**What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken?** \n",
    "> Below are observations of the biggest changes noticable compared to the basic driving agent:\n",
    "> * The biggest change that can be noticed is, the agent starts heading towards the destination even early in the process when almost no rules have been learned yet.\n",
    "> * There are no crashes that happen any more.\n",
    "> * The agent stops at certain points, when needed but doesn't halt at a stop when the light is green and there is no interfering traffic.\n",
    "> * The agent also learns that running over a red light is a bad choice and learns to either wait or pick a different route.\n",
    "\n",
    "\n",
    "**Why is this behavior occurring?**\n",
    "> * It becomes obvious that the agent tries to maximize the actions that are producing the highest reward.\n",
    "> * By adding a certain randomness to the algorithm (through a $\\epsilon$ greedy implementation) we're taking into account to make mistakes but avoid learning a semi-optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
