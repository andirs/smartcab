{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a self-driving car agent\n",
    "## Reinforcement Learning Project\n",
    "This documentation outlines the thought process behind training a smartcab to navigate on its own. We're applying reinforcement learning techniques for a self-driving agent in a simplified world to aid it in effectively reaching its destinations in the allotted time. \n",
    "\n",
    "First we'll investigate the given environment in which the agent operates in and instruct it, to execute very basic driving commands. Afterwards we'll exhibit the different states a smartcab can be in, analyzing its limited world and derive a Q-Learning algorithm, that will guide the agent towards its destination. At last we'll be going through several iterations in order to find the best configuration for our algorithm and the environment its operating in, to improve the results.\n",
    "\n",
    "## Description\n",
    "In the not-so-distant future, taxicab companies across the United States no longer employ human drivers to operate their fleet of vehicles. Instead, the taxicabs are operated by self-driving agents — known as smartcabs — to transport people from one location to another within the cities those companies operate. In major metropolitan areas, such as Chicago, New York City, and San Francisco, an increasing number of people have come to rely on smartcabs to get to where they need to go as safely and efficiently as possible. Although smartcabs have become the transport of choice, concerns have arose that a self-driving agent might not be as safe or efficient as human drivers, particularly when considering city traffic lights and other vehicles. To alleviate these concerns, your task as an employee for a national taxicab company is to use reinforcement learning techniques to construct a demonstration of a smartcab operating in real-time to prove that both safety and efficiency can be achieved.\n",
    "\n",
    "## Software Requirements\n",
    "This project uses the following software and Python libraries:\n",
    "\n",
    "* [Python 2.7](https://www.python.org/download/releases/2.7/)\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [PyGame](http://pygame.org/)\n",
    "    * **Helpful links for installing PyGame:**\n",
    "    * [Getting Started](https://www.pygame.org/wiki/GettingStarted)\n",
    "    * [PyGame Information](http://www.pygame.org/wiki/info)\n",
    "    * [Google Group](https://groups.google.com/forum/#!forum/pygame-mirror-on-google-groups)\n",
    "    * [PyGame subreddit](https://www.reddit.com/r/pygame/)\n",
    "    \n",
    "## Definitions\n",
    "\n",
    "### Environment\n",
    "The smartcab operates in an ideal, simplified, grid-like city (see image below). Roads are established on two major axis (North-South & East-West). There are other vehicles on the road but the city is abstracted to a point where there are no other obstacles such as traffic jams, construction sights or other heterogenous agents such as pedestrians. However there are certain rules the city runs on, which are alike to our street system. At each intersection there is a traffic light that either allows traffic in the North-South direction or the East-West direction.\n",
    "\n",
    "![Smart Cab Environment](smartcab_screenshot.png)\n",
    "\n",
    "**Following rules guide the traffic:**\n",
    "* On a green light, a left turn is permitted if there is no oncoming traffic making a right turn or coming straight through the intersection.\n",
    "* On a red light, a right turn is permitted if no oncoming traffic is approaching from your left through the intersection.\n",
    "\n",
    "### Inputs and Outputs\n",
    "Very much alike to modern hail-a-cab applications the smartcab will have a route assigned based on t he passengers' starting location and the destination. The route is split at each intersection into waypoints, and for simplicity purposes the the smartcab is at some intersection at any instant in the world. \n",
    "\n",
    "Therefore, the next waypoint to the destination, assuming the destination has not already been reached, is one intersection away in one direction (North, South, East, or West). \n",
    "\n",
    "### States\n",
    "The smartcab has only an egocentric view of the intersection it is at and can therefore use following information: \n",
    "\n",
    "* The state of the traffic light for its direction of movement: `['green', 'red']`\n",
    "* Whether there is a vehicle at the intersection for each of the oncoming directions: `['left', 'right', 'oncoming']`\n",
    "\n",
    "### Actions\n",
    "For each action, the smartcab has one of the following options:\n",
    "\n",
    "* Idle at the intersection\n",
    "* Drive to the next intersection to either of the directions, which offers a set of following actions: `[None, 'left', 'right', 'forward']`\n",
    "\n",
    "### Deadline\n",
    "The smartcab has to get to its final destination in a given time. With each action taken, this time decreases. If the allotted time becomes zero before reaching the destination, the trip has failed.\n",
    "\n",
    "### Rewards, Penalties and Goal\n",
    "Smartcabs receive rewards for each successfully completed trip and smaller rewards for each action they execute successfully that obeys traffic rules. For any incorrect action a small penalty will be given and violating traffic rules or causing traffic accidents result in a high penalty. Based on the rewards and penalties the smartcab receives, the self-driving agent implementation will learn an optimal policy for driving on the city roads while obeying traffic rules, avoiding accidents, and reaching passengers' destinations in the allotted time.\n",
    "\n",
    "## Implementing a Basic Driving Agent\n",
    "In order to get going, we'll implement a basic driving agent that chooses from all possible actions one at random and drives through the city streets. \n",
    "In order to do so, we set `enforce_deadline` to `False` on line `47` and implement the following code in `agent.py` on line `30`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick a random action from set of possible actions. \n",
    "import random\n",
    "action = random.choice([None, 'forward', 'left', 'right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smartcab performs following 5 actions: ['right', 'forward', None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# Quick code to demonstrate effect of actions allocation. \n",
    "n = 5\n",
    "action_dict = []\n",
    "for _ in range(n):\n",
    "    action_dict.append(random.choice([None, 'forward', 'left', 'right']))\n",
    "print \"Smartcab performs following\", n, \"actions:\", action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "There are a couple interesting observations when you run the smartcab with this source of randomness. \n",
    "\n",
    "While performing random actions there are a couple observations one can make:\n",
    "* When crashing, cars will restart at a different location and start to perform actions again.\n",
    "* There are 3 other agents or cars in play.\n",
    "* With n $\\rightarrow \\infty$ (through `enforce_deadline = False`) the agent reaches the goal eventually if it doesn't crash into another vehicle.\n",
    "* There is a reward system in place that shows the immediate gratification of an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inform the Driving Agent\n",
    "Now that the driving agent is capable of moving around in the environment, the next task is to identify a set of states that are appropriate for modeling the smartcab and environment. \n",
    "\n",
    "The main source of state variables are the current inputs at the intersection, but given our set of rules not all may require representation. The goal is, to process the inputs and update the agent's current state at each waypoint using the `self.state` variable. In order to check our performance we'll continue with the simulation deadline enforcement `enforce_deadline` being set to `False`.\n",
    "\n",
    "#### All information available\n",
    "Let's take a look at all information we've got available in our environment and talk through the possibilities that come with each of these inputs.\n",
    "\n",
    "##### Traffic Lights\n",
    "As mentioned above, we have an indicator that shows what color the traffic light at any given position of the agent has. This can take the values `green` and `red`. We find this information in the inputs dictionary and can call it with `inputs['light']`. Given our set of rules, this is a highly useful indicator for our smartcab to decide whether it should perform a certain action or not. This information will be included in our set of state variables.\n",
    "\n",
    "* `inputs['light'] = ['red', 'green']`\n",
    "\n",
    "##### Traffic\n",
    "Our agent can detect, whether there is a vehicle at the intersection for each of the oncoming directions and what action this other agent is performing. Given the set of rules that apply on US streets we have following information that we need to especially emphasize:\n",
    "\n",
    "* If the traffic light is green:\n",
    "    * Is there oncoming traffic going in my direction? ($\\rightarrow$ can't perform a left turn)\n",
    "* If the traffic light is red:\n",
    "    * Is there traffic coming from the left side? ($\\rightarrow$ can't make a right turn)\n",
    "\n",
    "However since we want to let our agent learn these rules through reinforcement and gratification we are going to add all the information about traffic in our set of states. This adds following set to the list: \n",
    "\n",
    "* `inputs['oncoming'] = [None, 'left', 'right', 'forward']`\n",
    "* `inputs['left'] = [None, 'left', 'right', 'forward']`\n",
    "* `inputs['right'] = [None, 'left', 'right', 'forward']`\n",
    "\n",
    "##### Waypoints\n",
    "The decision, which step to perform next is essential to learning if some action is good or bad, given the state. Especially in reinforcement learning. If you compare it to a simple but hurtful early life analogy, this is where our agent will understand if the cooktop is hot or not.\n",
    "\n",
    "* `self.next_waypoint = ['forward', 'left', 'right']`\n",
    "\n",
    "##### Deadline\n",
    "One could argue that the time that is available for the smartcab to perform its delivery describes the state its in. We can think about classic situations where cab drivers are pushed to their limits and into breaking the law in order to get a passenger faster to the destination. Since we're building this system though, to perform optimal results without breaking the law we should discard this input though. \n",
    "Another valid concern is the number of possible combinations. Given that the deadline reduces by `1` on each action we perform an average of 40 to 50 actions each run, this would add a multiplier of `len(deadline)` to the state combinations that we already have. With only 100 training runs including around 40 to 50 actions in each run this might be a state input that rather distracts the learner from learning what's essential for the task.\n",
    "\n",
    "##### The final states for our Q-Learner\n",
    "To summarize, we're planning on including following inputs to the list of possible state combinations:\n",
    "* `inputs['light'] = ['red', 'green']` $\\rightarrow 2$\n",
    "* `inputs['oncoming'] = [None, 'left', 'right', 'forward']` $\\rightarrow 4$\n",
    "* `inputs['left'] = [None, 'left', 'right', 'forward']` $\\rightarrow 4$\n",
    "* `inputs['right'] = [None, 'left', 'right', 'forward']` $\\rightarrow 4$\n",
    "* `self.next_waypoint = ['forward', 'left', 'right']` $\\rightarrow 3$\n",
    "\n",
    "The total number of states therefore is the combination of all states:\n",
    "$$ 2 * 4 * 4 * 3 = 384 $$\n",
    "\n",
    "**Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state?**\n",
    "Given that we are dealing with around `5000` waypoints in our training runs (`100` runs * `~50` actions), the amount of `384` states seems to be a bit high. We could consider dropping `inputs['right']`, since given our rules if the traffic light is red and we want to make a right turn (which is allowed by US rules) this would be a correct action that wouldn't end up in an accident but all other actions would be incorrect and therefore should be learned as being incorrect. This would bring down the number of states to `96` but we would lose some flexibility, if we for example would change the rules slightly and appply it to a European traffic setting our Reinforcement Learner would learn incorrect rules. Since we'll be adding each state as a combination when it occures to a dictionary and calculate its given value we might not even end up evaluating and adding all `384` states. Therefore it should be alright to add all appearances out of `384` possible combinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implement a Q-Learning Driving Agent\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, based on the Q-values for the current state and action. Each action taken by the smartcab will produce a reward which depends on the state of the environment. The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, set the simulation deadline enforcement enforce_deadline to True. Run the simulation and observe how the smartcab moves about the environment in each trial.\n",
    "\n",
    "The formulas for updating Q-values can be found in this video.\n",
    "\n",
    "**What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken?** \n",
    "\n",
    "**Why is this behavior occurring?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
